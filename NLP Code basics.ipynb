{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e98a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK \n",
    "# https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f566cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c1eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a029f270",
   "metadata": {},
   "source": [
    "The most common lexicon normalization practices are :\n",
    "\n",
    "Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "lem.lemmatize(word, \"v\")\n",
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Standardization\n",
    "\n",
    "#Text data often contains words or phrases which are not present in any standard lexical dictionaries. \n",
    "# These pieces are not recognized by search engines and models.\n",
    "#Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. \n",
    "# With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed\n",
    "\n",
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"...\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) new_text = \" \".join(new_words) \n",
    "        return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47430a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntactic Parsing: Syntactical parsing involves the analysis of words in the sentence for grammar and their arrangementin a manner that shows the relationships among the words. \n",
    "# Dependency Grammar and \n",
    "# Part of Speech tags are the important attributes of text syntactics.\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print pos_tag(tokens)\n",
    "\n",
    "\n",
    "# POS tags are the basis of lemmatization process for converting a word to its base form (lemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER ( named entity recognization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate N gram features\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)\n",
    "# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical features - TF IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding \n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'science'))\n",
    "\n",
    "\n",
    "print(model['learning'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f367448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modelling\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim from gensim\n",
    "import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Similarity\n",
    "\n",
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d89e8f58",
   "metadata": {},
   "source": [
    "Phonetic Matching – A Phonetic matching algorithm takes a keyword as input (person’s name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar. It is very useful for searching large text corpuses, correcting spelling errors and matching relevant names. Soundex and Metaphone are two main phonetic algorithms used for this purpose. Python’s module Fuzzy is used to compute soundex strings for different words, for example –"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83b30505",
   "metadata": {},
   "source": [
    "import fuzzy \n",
    "soundex = fuzzy.Soundex(4) \n",
    "print soundex('ankit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzy \n",
    "soundex = fuzzy.Soundex(4) \n",
    "print soundex('ankit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97bcdf0f",
   "metadata": {},
   "source": [
    "Cosine Similarity – W hen the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. Following code converts a text to vectors (using term frequency) and applies cosine similarity to provide closeness among two text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c674350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis : https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\n",
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "text_1 = \"The book was a perfect balance between wrtiting style and plot.\"\n",
    "text_2 =  \"The pizza tastes terrible.\"\n",
    "sent_1 = sentiment.polarity_scores(text_1)\n",
    "sent_2 = sentiment.polarity_scores(text_2)\n",
    "print(\"Sentiment of text 1:\", sent_1)\n",
    "print(\"Sentiment of text 2:\", sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Loading the dataset\n",
    "data = pd.read_csv('Finance_data.csv')\n",
    "#Pre-Processing the text \n",
    "def cleaning(df, stop_words):\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "    # Replacing the digits/numbers\n",
    "    df['sentences'] = df['sentences'].str.replace('d', '')\n",
    "    # Removing stop words\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
    "    # Lemmatization\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "    return df\n",
    "stop_words = stopwords.words('english')\n",
    "data_cleaned = cleaning(data, stop_words)\n",
    "#Generating Embeddings using tokenizer\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(data_cleaned['verified_reviews'].values)\n",
    "X = tokenizer.texts_to_sequences(data_cleaned['verified_reviews'].values)\n",
    "X = pad_sequences(X)\n",
    "#Model Building\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(352, activation='LeakyReLU'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#Model Training\n",
    "model.fit(X_train, y_train, epochs = 20, batch_size=32, verbose =1)\n",
    "#Model Testing\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef42372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"It was the best of times.\", \"t was the worst of times.\"]\n",
    "sentiment_pipeline(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f30aa5df1968170fcb8276e358730b036a43d0c2cc915b8ce6f3f15da992e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
